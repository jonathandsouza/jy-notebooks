{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonathandsouza/jy-notebooks/blob/main/MLP1_1_Scikit_learn_and_LR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set()\n",
        "%matplotlib inline "
      ],
      "metadata": {
        "id": "0kvDEpxcZf1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using real-life data in Google Colab"
      ],
      "metadata": {
        "id": "Io2Gb383R6vR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this course we will be using some real-life datasets. Let's first briefly discuss how to use real-life data in Google Colab:"
      ],
      "metadata": {
        "id": "Y5XWxlawR-4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pandas supports reading all kinds of datasets, but in practice the most commonly used are probably excel and csv files.\n",
        "\n",
        "Since we work with Google Colab, there is an extra step involved: we also have to make the data available in this online environment.\n",
        "\n",
        "For that we use the code below, to mount our Google Drive."
      ],
      "metadata": {
        "id": "67G0SUf2SO4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "OzKeXmmnwn8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* You can then click on the folder icon, here on the left, to see your drive-folder.\n",
        "* You can now upload the dataset(s) you need to your Drive. You can then load the data into your notebook using the pandas read_csv and read_excel functions.\n",
        "\n",
        "You can find the path (in quotes) to which you need to refer by clicking on the three dots in your folder structure and choosing \"copy path\". You can then paste that path into your read function.\n",
        "\n",
        "I have put my folder datasets directly in my main folder on Google Drive:"
      ],
      "metadata": {
        "id": "AmqccZbpSXnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_iris = pd.read_csv(\"/content/drive/MyDrive/datasets/iris.csv\")\n",
        "\n",
        "df_iris.head(3)"
      ],
      "metadata": {
        "id": "TiJQqsrIShGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The folder with (open) datasets we use in this course can be found [here](https://drive.google.com/drive/folders/1QIgKvGSeltdzH6HMLlEM_ddYYObsrBO2?usp=sharing).\n",
        "\n",
        "So please download the entire folder and upload them to your own drive.\n",
        "Now we can start using the data."
      ],
      "metadata": {
        "id": "5AVsvD3XStMi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Churn case\n",
        "In this course we will have a returning case with regards to churning (=customers stop using a product/service). \n",
        "\n",
        "Our ultimate goal is to predict which Telco customers will churn. (but we will try some other predictions first)"
      ],
      "metadata": {
        "id": "md1CWfTn4Wmq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have a dataset (churn_data.csv) of 10,000 customers. We know these things about the customers:\n",
        "- What technology they use (e.g. 4G or cable)\n",
        "- Age\n",
        "- How long they have been a customer\n",
        "- How many times they called the helpdesk last year\n",
        "- The average monthly invoice amount\n",
        "- A churn indicator: indicates whether the customer churned\n",
        "\n",
        "Let's first take a look at the data:"
      ],
      "metadata": {
        "id": "hYrtsUDswlbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_churn = pd.read_csv(\"/content/drive/MyDrive/datasets/churn_data.csv\", delimiter=\";\", decimal=\",\")\n",
        "df_churn.head(10)"
      ],
      "metadata": {
        "id": "-XMSk3BMZvzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_churn.describe()"
      ],
      "metadata": {
        "id": "dANtL0ofzyj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because we're just starting out, we will start with a little experiment: trying to predict the AverageBill of customers, only based on their Age and number of SupportCallsLastYear."
      ],
      "metadata": {
        "id": "OZ4ZKFTp4nfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Modelling data with scikitlearn - the basics\n",
        "\n",
        "Among other things, with scikitlearn we can:\n",
        "* split datasets into training data and test data\n",
        "* train different types of models\n",
        "* analyze the performance of models\n",
        "* optimize hyperparameters\n",
        "* select features"
      ],
      "metadata": {
        "id": "ID0H2vygXmpe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Below we split the columns into two categories:\n",
        "*   the features, these are the independent variables\n",
        "*   the target, this is the variable we want to predict (AverageBill)\n",
        "\n",
        "So the question is: can we predict the bill, based on the given characteristics?"
      ],
      "metadata": {
        "id": "RXbfmGFxaI_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = df_churn[[\"Age\", \"SupportCallsLastYear\"]]\n",
        "target = df_churn.AverageBill"
      ],
      "metadata": {
        "id": "KjSPwBhXZjts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make sure that a trained model is suitable for making predictions on new data, we usually split a dataset into a training set and a test set.\n",
        "\n",
        "We train the model on the training set and then we test the performance on the test set."
      ],
      "metadata": {
        "id": "c9iACtfmZAI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import sklearn train_test_split functionality\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "0nJEnzYtXl2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now going to split the data into different sets:\n",
        "* the features_train, consists of the feature columns, and from those we take the largest part of the rows to use for training\n",
        "* the features_test, again the feature columns, but these rows go into the test set\n",
        "* target_train, this is the target and again the rows we use for training\n",
        "* target_test, the remaining rows for the test set\n",
        "\n",
        "Scikitlearn's train_test_split function returns these four sets.\n",
        "\n",
        "We input the features and target into the train_test_split() function, and we indicate that the size of the test set is 20% of the total. \n",
        "\n",
        "NB: We set a random_state here. The random_state takes care of random selection of the rows for test vs. train. Fixing this value ensures that you can reproduce this selection the next time you run this notebook."
      ],
      "metadata": {
        "id": "OZWIoKt3aH0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features_train, features_test, target_train, target_test = train_test_split(features, \n",
        "                                                                            target, \n",
        "                                                                            test_size=0.2, \n",
        "                                                                            random_state=1);"
      ],
      "metadata": {
        "id": "FzOy4PKGZx8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's double check whether the split went well."
      ],
      "metadata": {
        "id": "3YDkmeIKbcp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features_train.shape, features_test.shape, target_train.shape, target_test.shape"
      ],
      "metadata": {
        "id": "uKcKgiZlZ1Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that of the 10000 entries in the dataset, 2000 have ended up in the test set. That is indeed exactly 20%. Furthermore, we see that both features datasets contain 2 columns, and that for both target datasets the result appears to be a Series object (1 column).\n",
        "\n",
        "Note that it is quite possible that by splitting the data sets the distributions of the training set and the test set no longer correspond. To ensure that the distribution matches after splitting, you can use \"stratified sampling\" (which we will not discuss here)."
      ],
      "metadata": {
        "id": "Ou4abMz_bsPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features_train.head()"
      ],
      "metadata": {
        "id": "rp_L01I8Z9hK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_train.head()"
      ],
      "metadata": {
        "id": "zDHMvcOkZ_Y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training a model"
      ],
      "metadata": {
        "id": "XWpEAHIncYbj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to initialize our first regression model with default settings.\n",
        "\n",
        "Each regressor has:\n",
        "* a `fit(X, y)` function to fit the model to the data\n",
        "* a `predict(X)` function that uses the fitted model to do predictions\n",
        "\n",
        "The `fit(X,y)` function expects a features-dataframe (with every feature in a column, and each item in a single row), and after the comma a Series containing the target-variable.\n",
        "\n",
        "The `predict(X)` function expects a dataframe containing the samples to predict\n",
        "\n",
        "Passing a pandas dataframe works fine, but sometimes it causes problems and you have to make it into a numpy matrix. You can do this in the following way: `matrix = df.values`"
      ],
      "metadata": {
        "id": "dZL-RkRacX83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize our model to be a linear regression model (using build in sklearn functionality)\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lr = LinearRegression()"
      ],
      "metadata": {
        "id": "SNxXuwVBcWQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets train the LinearRegression model on the splitted churn dataset. As input for the fit() function, we use the created features_train and target_train variables."
      ],
      "metadata": {
        "id": "I3ZiEZiiciDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr.fit(features_train, target_train)"
      ],
      "metadata": {
        "id": "pGOyZFQvcgwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SciKitLearn has now created a model that is stored in 'lr'. This is the trained regression model that tries to predict the AverageBill on the basis of the given features."
      ],
      "metadata": {
        "id": "09Pr9FvncnH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Using the model for predictions on new data\n",
        "\n",
        "We can use the trained model for predictions on new data from the test set."
      ],
      "metadata": {
        "id": "cmuAQrd8c0cK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = lr.predict(features_test)\n",
        "preds"
      ],
      "metadata": {
        "id": "idkejxfycmqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds.shape"
      ],
      "metadata": {
        "id": "wx1FzFKa9osc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the model has made a prediction for each of the 2000 entries in the test set. We get this back in the form of a Numpy Array (so not as a series!)\n",
        "\n",
        "But is the model right?\n",
        "\n",
        "We also know what the real value was for these 2000 entries. So we can compare the results of the model with the real answer to see how well the model performs on new data."
      ],
      "metadata": {
        "id": "1YmTr3nhdHPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
      ],
      "metadata": {
        "id": "BVh9HrrSdFYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"RMSE\",mean_squared_error(target_test,preds, squared=False))\n",
        "print(\"MAE\",mean_absolute_error(target_test,preds))\n",
        "print(\"R2\",r2_score(target_test,preds))"
      ],
      "metadata": {
        "id": "URgLYuUMdaMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well.. not the best model. A mean average error of almost half of the average bill. \n",
        "\n",
        "And looking at the low R-squared score we can also see our model can not explain variations in AverageBill very well."
      ],
      "metadata": {
        "id": "0AK3mAaHBgdt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And actually, that was to be expected. We didn't do any proper research upfront. For example, we did not check if there is even a linear relationship between the variables. \n",
        "\n",
        "But for now, it is ok, we just created our first Machine Learning model!"
      ],
      "metadata": {
        "id": "xcvxR1KJCtNf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using baseline scores\n",
        "Interpreting performance scores can be quite tricky. But what might help is using a baseline. For example a baseline of always using the mean value as a prediction, or the median value. \n",
        "You can create such a dummy-model and see the performance scores of using such a strategy.\n",
        "Those dummy performance scores can be a usefull starting point for comparison with your actual model."
      ],
      "metadata": {
        "id": "fAjwxTxID8sv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.dummy import DummyRegressor\n",
        "# let's try always using the mean value\n",
        "dummy = DummyRegressor(strategy=\"mean\")\n",
        "\n",
        "dummy.fit(features_train, target_train)\n",
        "dummy_preds = dummy.predict(features_test)\n",
        "print(\"MAE\",mean_absolute_error(target_test,dummy_preds))\n",
        "print(\"R2\",r2_score(target_test,dummy_preds))"
      ],
      "metadata": {
        "id": "OjUEpJSmmxrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seams our model at least outperforms the dummy regressor. So predictions are more accurate than just using the mean AverageBill all the time."
      ],
      "metadata": {
        "id": "-qoz2m-_nEUt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 1.1"
      ],
      "metadata": {
        "id": "OT4t15FiEJJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: making a LR model from scratch**\n",
        "\n",
        "Below we first fetch an existing dataset to use in this question.\n",
        "\n",
        "It is a dataset on california houses and their value (average house value in units of 100,000)."
      ],
      "metadata": {
        "id": "laSve_kbqyyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing;\n",
        "\n",
        "data = fetch_california_housing(as_frame=True);\n",
        "\n",
        "# Get the features and the target\n",
        "df_features = data['data']\n",
        "df_target = data['target']\n",
        "\n",
        "\n",
        "print('features shape',df_features.shape);\n",
        "print('target shape',df_target.shape);\n",
        "\n",
        "\n",
        "print('Independent variables:\\n',df_features.describe())\n",
        "print('\\nTarget variable:\\n',df_target.describe());\n",
        "print(df_features.head());\n"
      ],
      "metadata": {
        "id": "2Lno8bSthj6g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44262eca-24ab-47a2-cd38-a3e20fb57b4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features shape (20640, 8)\n",
            "target shape (20640,)\n",
            "Independent variables:\n",
            "              MedInc      HouseAge      AveRooms     AveBedrms    Population  \\\n",
            "count  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000   \n",
            "mean       3.870671     28.639486      5.429000      1.096675   1425.476744   \n",
            "std        1.899822     12.585558      2.474173      0.473911   1132.462122   \n",
            "min        0.499900      1.000000      0.846154      0.333333      3.000000   \n",
            "25%        2.563400     18.000000      4.440716      1.006079    787.000000   \n",
            "50%        3.534800     29.000000      5.229129      1.048780   1166.000000   \n",
            "75%        4.743250     37.000000      6.052381      1.099526   1725.000000   \n",
            "max       15.000100     52.000000    141.909091     34.066667  35682.000000   \n",
            "\n",
            "           AveOccup      Latitude     Longitude  \n",
            "count  20640.000000  20640.000000  20640.000000  \n",
            "mean       3.070655     35.631861   -119.569704  \n",
            "std       10.386050      2.135952      2.003532  \n",
            "min        0.692308     32.540000   -124.350000  \n",
            "25%        2.429741     33.930000   -121.800000  \n",
            "50%        2.818116     34.260000   -118.490000  \n",
            "75%        3.282261     37.710000   -118.010000  \n",
            "max     1243.333333     41.950000   -114.310000  \n",
            "\n",
            "Target variable:\n",
            " count    20640.000000\n",
            "mean         2.068558\n",
            "std          1.153956\n",
            "min          0.149990\n",
            "25%          1.196000\n",
            "50%          1.797000\n",
            "75%          2.647250\n",
            "max          5.000010\n",
            "Name: MedHouseVal, dtype: float64\n",
            "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
            "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
            "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
            "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
            "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
            "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
            "\n",
            "   Longitude  \n",
            "0    -122.23  \n",
            "1    -122.22  \n",
            "2    -122.24  \n",
            "3    -122.25  \n",
            "4    -122.25  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and evaluate a Linear Regression model on this dataset.\n",
        "\n",
        "Remember to train_test_split(), and you might want to create a dummy model to compare your scores to."
      ],
      "metadata": {
        "id": "0wwUf6CPq-lG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Split the data in trainging & tesgting; \n",
        "\n",
        "from sklearn.model_selection import train_test_split;\n",
        "\n",
        "training_feature, testing_feature, training_target, testing_target = train_test_split(df_features, df_target, test_size=0.2, random_state=1);\n",
        "\n",
        "print(training_feature.shape, training_target.shape);\n"
      ],
      "metadata": {
        "id": "99nry4a9H89T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "028b6045-49a7-462b-d113-3c6df027e3fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16512, 8) (16512,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a model from \n",
        "\n",
        "from sklearn.linear_model import LinearRegression;\n",
        "\n",
        "lr_model = LinearRegression();\n",
        "\n",
        "lr_model.fit(training_feature, training_target);\n"
      ],
      "metadata": {
        "id": "fKvrYx2fFPG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Model to gain prediction. \n",
        "\n",
        "predictions = lr_model.predict(testing_feature);\n"
      ],
      "metadata": {
        "id": "v4q4Tn6yHtAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test accuracy of the data \n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score;\n",
        "\n",
        "def evaluateMetrics(testing_target, predictions):\n",
        "    print(\"RMSE:\", mean_squared_error(testing_target, predictions, squared=False));\n",
        "    print(\"MAE:\", mean_absolute_error(testing_target, predictions));\n",
        "    print(\"R2:\", r2_score(testing_target, predictions));\n",
        "\n",
        "evaluateMetrics(testing_target, predictions);\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RygKFGsPIrWj",
        "outputId": "ea7fdef3-7d25-4e9c-87c2-caa1a9c41a81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 0.7274202599183853\n",
            "MAE: 0.5328685121247797\n",
            "R2: 0.596596837481235\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dummy \n",
        "\n",
        "from sklearn.dummy import DummyRegressor;\n",
        "\n",
        "dummy_model = DummyRegressor();\n",
        "\n",
        "dummy_model.fit(training_feature, training_target);\n",
        "\n",
        "dummy_preds = dummy_model.predict(testing_feature);\n",
        "\n",
        "print(\"RMSE:\", mean_squared_error(testing_target, dummy_preds));\n",
        "print(\"MAE:\", mean_absolute_error(testing_target, dummy_preds));\n",
        "print(\"R2:\", r2_score(testing_target, dummy_preds));\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BokEA5CeJhlL",
        "outputId": "675a8acf-3ab3-42a7-8437-bc76685b3005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 1.3136235330402446\n",
            "MAE: 0.9090896299308748\n",
            "R2: -0.0014734336890012134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: improve model**\n",
        "\n",
        "Try improving the model performance by going back to the dataset and removing the outliers. \n",
        "After removing outliers, again divide the dataset into features and target, train_test_split, and train and evaluate your new model."
      ],
      "metadata": {
        "id": "FJtqhwqbJewk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EfeMil4uKPbc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}